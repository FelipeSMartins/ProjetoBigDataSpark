{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39bdc1b",
   "metadata": {},
   "source": [
    "# Análise por Eventos lendo direto do HDFS\n",
    "Este notebook conecta-se ao cluster Spark (standalone) e lê as janelas de eventos direto do HDFS, gerando gráficos por evento com linhas por **categoria**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ada48e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_608/2957464296.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import os, json, re\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "events_cfg = '/opt/config/events.json'  # montado no container\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName('EventAnalysisNotebook')\n",
    "         .master('spark://spark-master:7077')\n",
    "         .config('spark.hadoop.fs.defaultFS','hdfs://namenode:8020')\n",
    "         .getOrCreate())\n",
    "\n",
    "def slugify(name: str) -> str:\n",
    "    return re.sub(r'[^a-z0-9_\\-]', '-', name.lower().replace(' ', '-'))\n",
    "\n",
    "def pick_price_column_spark(columns):\n",
    "    if 'AdjClose' in columns: return 'AdjClose'\n",
    "    if 'Adj Close' in columns: return 'Adj Close'\n",
    "    return 'Close'\n",
    "\n",
    "def normalize_by_anchor_spark(df, anchor_date: str):\n",
    "    df = df.withColumn('date', F.to_date('date'))\n",
    "    price_col = pick_price_column_spark(df.columns)\n",
    "    # baseline: valor por categoria no dia da âncora; fallback: primeiro dia disponível\n",
    "    anchor_df = (df.filter(F.col('date') == F.lit(anchor_date))\n",
    "                   .groupBy('category').agg(F.first(price_col).alias('baseline')))\n",
    "    if anchor_df.rdd.isEmpty():\n",
    "        first_day = df.groupBy('category').agg(F.min('date').alias('min_date'))\n",
    "        anchor_df = (df.join(first_day, ['category'])\n",
    "                       .filter(F.col('date') == F.col('min_date'))\n",
    "                       .groupBy('category').agg(F.first(price_col).alias('baseline')))\n",
    "    out = (df.join(anchor_df, ['category'], 'left')\n",
    "             .withColumn('index', (F.col(price_col) / F.col('baseline')) * F.lit(100.0)))\n",
    "    return out\n",
    "\n",
    "def aggregate_for_plot(df):\n",
    "    return (df.groupBy('category','date')\n",
    "              .agg(F.mean('index').alias('index'))\n",
    "              .orderBy('date'))\n",
    "\n",
    "events = json.load(open(events_cfg, 'r'))\n",
    "events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd146dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ev in events:\n",
    "    name = ev['name']\n",
    "    anchor = ev['anchor_date']\n",
    "    slug = slugify(name)\n",
    "    print(f'Evento: {name} (âncora: {anchor})')\n",
    "\n",
    "    base = 'hdfs://namenode:8020/datasets/yahoo_finance/curated/event_windows'\n",
    "\n",
    "    win_paths = {\n",
    "        'pre': f\"{base}/{slug}/pre\",\n",
    "        'during': f\"{base}/{slug}/during\",\n",
    "        'post': f\"{base}/{slug}/post\"\n",
    "    }\n",
    "\n",
    "    dfs = {}\n",
    "    for w, p in win_paths.items():\n",
    "        sdf = spark.read.parquet(p)\n",
    "        sdf = normalize_by_anchor_spark(sdf, anchor)\n",
    "        dfs[w] = aggregate_for_plot(sdf).toPandas()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    for w, dfp in dfs.items():\n",
    "        for cat in sorted(dfp['category'].unique()):\n",
    "            dcat = dfp[dfp['category']==cat]\n",
    "            ax.plot(dcat['date'], dcat['index'], label=f'{cat} ({w})')\n",
    "    ax.axvline(datetime.strptime(anchor, '%Y-%m-%d'), color='black', linestyle='--', linewidth=1, label='âncora')\n",
    "    ax.set_title(name)\n",
    "    ax.set_ylabel('Índice (base=100)')\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    plt.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
